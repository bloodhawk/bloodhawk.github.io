At work recently I've had the opportunity to deploy Elasticsearch into our stack. In this post, I'd like to go over the process that led me to this decision.

Cars.com (Dealer Inspire) tracks inventory. For instance, with a car, we may have a make, model, stock #, drivetrain, images, etc. Most inventory items have 100+ properties. These properties are not always simple datatypes like Ints, Strings, etc. These datatypes can consist of lists of Strings, Nested Objects, etc.

Early on in the project, we decided to use a NoSQL document Store as our authoritative database for inventory. Immediately, one might ask why not just use a normal RDMS like MySQL or Postgres? The answer in my eyes was simple and still is. You can choose to normalize this data out in a traditional relational style, but to what end? What are you getting out of it? What I find is that you end up with something horizontally scaled out that proves to be cumbersome for most tasks. To load a single piece of the inventory, we must aggregate or join many other tables to get the complete picture.

However, It doesn't stop there. It was also requested early on to use GraphQL as our backend API interface. We chose this solution to cut down on the payloads sent to web and mobile -- mobile especially. If you are not already familiar, GraphQL allows you to request payloads with the specific fields you want against a standard interface. If you have not worked with it, I highly recommend researching it. Back to the story. What happens when GraphQL asks us to get specific fields on a piece of inventory and nothing else? With a traditional RDBMS solution, we generally will have sophisticated control flow logic to ensure our SQL query gets only what we need. You may say, wouldn't we add a field to our SELECT statement? No. Not if you are normalizing your data and don't have a 100+ column database table. And if you do, at that point you should probably look into using a document store. With a document store, we run a projection on it telling it only to return specific fields for an object. And, because this object is readily available to us and stored in one contiguous portion of memory, it is very performant. In fact, in most cases, much more performant than an RDMS for this type of operation.

On to our next reason, a document store is beneficial. It is adaptable to change. We can add and remove fields without much of a fuss and complex schema migrations. This is quite valuable, especially when you are developing a project where all the properties of an object are not known and may never be wholly known.

Now at this point, you may be thinking I hate RDMS systems. And you would be wrong. I love RDMS when the data is relational. It just makes sense. We use a document store in tangent with an RDMS. We use RDMS for all of our relational structures like users, user permissions, dealerships, etc. This data is highly relational, and it just makes sense to normalize it. However, a piece of inventory is not the same. It is an object with many properties that we want to be able to pass around easily without having to refer to many tables and other unnecessary complex structures. Much like an instance of a class in many programming languages, the document in a document store is a distinct piece of data we want to be able to pass around, serialize, and process without fuss and headache. There are many document stores on the market, but by far, one of the most successful is MongoDB. For our project, we chose MongoDB due to its excellent community support and reliable 3rd party managed services.

MongoDB is great for all of the things mentioned above. It is also fast on doing complex queries against documents. Internally MongoDB flattens all data structures so that everything nested is easily indexed and can be queried on. However, something MongoDB cannot do is fast index intersections and robust full-text search.

MongoDB didn't have index intersections until version 2.6. Previous to 2.6 MongoDB could only use a single index to fulfill most queries. If anyone has dealt with highly complex filters and running those against databases, you can understand just how big of a problem this can be. I have found through trial and error that MongoDB's index intersection still isn't optimal or as performant as I've seen in other database engines. Understandable. MongoDB is not that old, and the feature was introduced not that long ago in the grand scheme of things. I'm not harping on MongoDB. For such a young project, it is very successful and has many things going for it. These more advanced features will improve over time.

MongoDB does have full-text search features, but it is very limited in what types of queries you can do with it. It is also very limiting in the kinds of pre/post processing you can do to text (keywords, stop words, stemming, etc.).

We now had a real problem. We have highly complex filters, and we need to do advanced full-text search queries. What do we do? Enter ElasticSearch.

ElasticSearch can store things much as a document store can. Things are documents. But looks can be deceiving. What happens under the hood is very different. At this point, it would be good to talk about the underlying architecture of MongoDB and ElasticSearch.

ElasticSearch (which uses Lucene under the hood, which we will get to) uses vector space models and inverted indexes for retrieval of data. These methods are highly efficient ways of making record comparison against a query. Due to the nature of this data structure, when you send a query to ElasticSearch, it already knows the answer to the question for the most part. Most of any "work" it needs to do may be ranking the results if you are asking for that type of information. These data structures also scale very well. And by well, I mean billions of records with 10ms response times. Now there is a trade-off. There are always trade-offs. You pay for this with longer write speeds and generally larger hard drives to house these indexes.

MongoDB's approach is akin to a general purpose data store. It will compare binary JSON documents using standard single indexes, compound indexes, etc. Due to this, you need to craft your indexes to match the queries carefully you will be running -- not something easy or even possible in some cases when perhaps the entirety of the document needs to be searchable. On the other hand, MongoDB is a valid database which at version 4.0 is now fully ACID compliant.

Now you may be asking: If ElasticSearch is not ACID compliant and isn't a traditional data store, then why are we using it? Well, first, you need to drop the notion of ElasticSearch as a database. It isn't a database. It is a search engine that is built around the concept of eventual consistency and is nowhere near being ACID compliant. Neither should it be. ElasticSearch was not made to be ACID compliant. If it were, it would likely degrade performance. ElasticSearch is good for one thing: searching on many documents fast. Indexes in ElasticSearch are temporary; they may be deleted and added back on a whim. We may have many indexes for the same data, just indexed slightly differently. What ElasticSearch provides is a rock solid API to create fast indexes to search on. This API is all backed by Lucene, otherwise known as Apache Lucene. Lucene was released back in 1999. It has had two decades to figure out how to solve the problem of search and solve it well.

If you are still interested at this point, I recommend starting with the documentation for ElasticSearch. It is wonderful. You can find it [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html).

Good luck with your search adventure. If you need help, I urge you to see if ElasticSearch can assist with your goals.
